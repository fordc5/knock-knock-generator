\documentclass[twoside,twocolumn]{article}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} 
\linespread{1.02} 
\usepackage{microtype} 
\usepackage{supertabular}
\usepackage[english]{babel} 

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} 
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} 
\usepackage{booktabs} 

\usepackage{lettrine} 

\usepackage{enumitem} 
\setlist[itemize]{noitemsep} 

\usepackage{abstract} 
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{titlesec} 
\renewcommand\thesection{\Roman{section}} 
\renewcommand\thesubsection{\roman{subsection}} 
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} 
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} 

\usepackage{fancyhdr} 
\pagestyle{fancy} 
\fancyhead{}
\fancyfoot{} 
\fancyhead[C]{Joke's On You $\bullet$ May 2019 $\bullet$ Ford, Magee} % Custom header text
\fancyfoot[RO,LE]{\thepage} 

\usepackage{titling} 
\usepackage{hyperref} 

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip}

\pretitle{\begin{center}\Huge\bfseries} 
\posttitle{\end{center}} 
\title{Joke's On You: An Exercise in Joke Generation} 
\author{%
\textsc{Connor Ford and Gabe Magee} \\[1ex] 
\normalsize Pomona College \\ 
\normalsize connor.ford@pomona.edu, gabe.magee@pomona.edu
}
\date{\today}


\begin{document}


\maketitle

\section*{Abstract}

Joke generation is a difficult task for humans and machines alike. It necessitates context, timing, and wordplay to elicit a laugh. We consider a subclass of 'knock-knock' jokes to simplify the generation and use a variety of straight-forward algorithms to create a small number of jokes with origins from movie scripts and the web.

\section{Introduction}

There have a few somewhat successful approaches to joke generation. \footnote{Cai, J., and Ehrhardt, N. (2013). Is This A Joke?. Yoshida K., et al. (2018). Neural Joking Machine: Humorous image captioning. Mihalcea, R. and Strapparava, C. (2006). Learning to Laugh (Automatically): Computational Models for Humor Recognition.} 
These jokes typically follow a certain rigid structure such as call-and-response. But others have trained more intelligent models on large corpora of data scraped from Reddit or Twitter. These have less associated structure and generally see more mixed results.
We want to consider a less common joke-type in the current literature: the 'knock-knock' joke. Considering this joke type has a couple main advantages.
(i) It follows a rigid structure. To illustrate the format, we annotate the following classic 'knock-knock' joke (not generated).
\begin{center}
\begin{enumerate}
\item[A:] Knock knock.
\item[B:] Who's there?
\item[A:] Cash. [\emph{Token}]
\item[B:] Cash who? [\emph{Token + who $\approx$ search word}]
\item[A:] No thanks, I'll have the peanuts [\emph{Play on search word}]
\end{enumerate}
\end{center}

\noindent (ii) There is a discrete set of 'knock-knock' joke subtypes \footnote{Taylor, J. (2004). Computationally recognizing wordplay in jokes.}.
One of which, as Taylor identifies, is word play on the \emph{token}. We pursue this specific type because it allows us to take a more straightforward, algorithmic approach to generation.
The algorithm looks like:
\begin{center}
    \begin{enumerate}
    \item[(a)] Generate \emph{Token} from dictionary/corpus by identifying \emph{search words} that end in the 'who'/'ew' sound and extracting the root of these words.
    \item[(b)] Use corpus to generate responses based on the \emph{search word}.
    \end{enumerate}
\end{center}


%------------------------------------------------

\section{Methods}

\subsection{Generating Tokens}

We used a corpus of $\sim 300k$ words and found words ending in the following substrings: ["who", "ew", "ewe", "ooo", "ooh", "oo", "hue", "ue", "eau", "eww", "hu"].  We cross-referenced the \emph{tokens} with another English language dictionary. This cross-referencing only generates a list of 9 potential english words. To extend this, while sacrificing a bit of quality, we cross-referenced tokens with a pronunciation dataset based on a CMU pronunciation dictionary with words ending in ["UW0", "UW1", "UW2"]. This outputs a list of about 68 english \emph{search words} but the quality of the \emph{tokens} took a hit. For example, it generates \emph{tokens} such as 'sh' corresponding to \emph{search word} 'shew'. 

\subsection{Generating Responses}
We decided to generate a response to a joke in one of two ways:
\begin{enumerate}[label=(\roman*)]
\item Using an n-gram approach learned from a Corpus of Contemporary American English web dataset. This required stringing together phrases to artificially construct a response.
\item Using a 'sentence following' approach from a movie script dataset. We find the occurrence of a word in the movie script data set then return the next sentence in the dialogue or the next line of dialogue.
\end{enumerate}
%------------------------------------------------

\section{Results}

Below is a selection of jokes from four categories. The tokens either come from \_cmu or \_2d (where 
\_2d is the list of 9 words and \_cmu the list of 68 words referenced in section II.i). The responses come from either the n-gram method, the next dialogue movie script method, or the next sentence movie script method. 

\begin{center}
\begin{supertabular}{lp{10mm}p{33mm}}
\toprule
token & search word & response\\
\midrule
\multicolumn{3}{r}{\textbf{\_cmu x n-gram}} \\
\cmidrule(r){1-3}
bl & blew & Blew out the candles. \\
val & value & Value of its currency.\\
\midrule
\multicolumn{3}{r}{\textbf{\_cmu x next dialogue}} \\
\cmidrule(r){1-3}
curf & curfew & It won't happen again. \\
arg & argue & That's up to the judge, Gus. It's not your problem anymore. \\
\midrule
\multicolumn{3}{r}{\textbf{\_2d x next dialogue}} \\
\cmidrule(r){1-3}
rev & revue & That job's gonna cost you a hundred bucks. \\
resid & residue & What does that mean? \\
\midrule
\multicolumn{3}{r}{\textbf{\_2d x next sentence}} \\
\cmidrule(r){1-3}
tab & taboo & Set forth in the Sacred Scrolls. \\
mild & mildew & And is that it? More or less. \\
\bottomrule
\end{supertabular}
\end{center}

%------------------------------------------------

\section{Discussion}

\subsection{Why Human Evaluation}

We used human evaluation to assess the four joke types because it is more difficult to evaluate our results with traditional quantitative metrics. A score generated from collective human reactions to jokes generated may be a good way to evaluate the models.

\subsection{Evaluation Methods}

We surveyed just over 100 people, each evaluating a random subset of 12 jokes (3 of each type). We asked them to score the humor and coherence of a joke on a scale of 1 (best) to 4 (worst). In the table below, the results are listed as: humor (coherence) with respect to response mean, st. dev, and percentage of responses with highest rating.

\subsection{Evaluation Results}

\begin{center}
    \begin{tabular}{ p{2.0cm} | p{1.2cm}| p{1.2cm} | p{0.9cm} }
    model & mean & sd & pct. 1 \\
    \hline
    \_cmu x n-gram & 3.2 (3.1) & 1.0 (1.0) & 7 (7) \\ 
    \hline
    \_cmu x next dialogue & 3.1 (3.4) & 1.1 (0.9) & 13 (5) \\ 
    \hline
    \_2d x next dialogue & 2.9 (3.1) & 1.1 (1.0) & 15 (6) \\ 
    \hline
    \_2d x next sentence & 3.3 (3.5) & 1.0 (0.8) & 7 (2) \\ 
    \end{tabular}
\end{center}
    
Generally, we see that humor outperforms coherence except in the bi-gram case. Moreover, although not statistically significantly, there appears to be a slight bias towards favoring the movie script generation, perhaps because it provides a more conversational, colloquial feel to the flow.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template


\bibitem[Yoshida, Kota et al, 2018]{Yoshida:2018dg}
Kota Yoshida and Munetaka Minoguchi and Kenichiro Wani and Akio Nakamura and Hirokatsu Kataoka. (2018).
\newblock Neural Joking Machine: Humorous image captioning.
\newblock {\em CoRR}, abs/1805.11850.

\bibitem[Cai, J., and Ehrhardt, N., 2013]{Cai:2013dg}
Cai, J., and Ehrhardt, N. (2013). 
\newblock Is This A Joke?.

\bibitem[Mihalcea, R. and Strapparava, C, 2006]{Mihalcea:2006dg}
Mihalcea, R. and Strapparava, C. (2006), 
\newblock Learning to Laugh (Automatically): Computational Models for Humor Recognition.
\newblock{\em Computational Intelligence}, 22: 126-142. doi:10.1111/j.1467-8640.2006.00278.x

\bibitem[Taylor, 2004]{Taylor:2004dg}
Taylor, J. (2006), 
\newblock Computationally recognizing wordplay in jokes
\newblock{\em Cognitive Science}

\bibitem[Danescu-Niculescu-Mizil, Cristian and Lee, Lillian, 2011]{Danescu:2011}
\newblock Proceedings of the 2nd Workshop on Cognitive Modeling and Computational Linguistics "Chameleons in imagined conversations: A new approach to understanding coordination of linguistic style in dialogs"
  \newblock {\em Association for Computational Linguistics}76--87

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}