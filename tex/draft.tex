\documentclass[twoside,twocolumn]{article}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} 
\linespread{1.05} 
\usepackage{microtype} 
\usepackage{supertabular}
\usepackage[english]{babel} 

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} 
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} 
\usepackage{booktabs} 

\usepackage{lettrine} 

\usepackage{enumitem} 
\setlist[itemize]{noitemsep} 

\usepackage{abstract} 
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{titlesec} 
\renewcommand\thesection{\Roman{section}} 
\renewcommand\thesubsection{\roman{subsection}} 
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} 
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} 

\usepackage{fancyhdr} 
\pagestyle{fancy} 
\fancyhead{}
\fancyfoot{} 
\fancyhead[C]{Joke's On You $\bullet$ May 2019 $\bullet$ Ford, Magee} % Custom header text
\fancyfoot[RO,LE]{\thepage} 

\usepackage{titling} 
\usepackage{hyperref} 

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip}

\pretitle{\begin{center}\Huge\bfseries} 
\posttitle{\end{center}} 
\title{Joke's On You: An Exercise in Joke Generation} 
\author{%
\textsc{Connor Ford and Gabe Magee} \\[1ex] 
\normalsize Pomona College \\ 
\normalsize connor.ford@pomona.edu, gabe.magee@pomona.edu
}
\date{\today}


\begin{document}


\maketitle

\section*{Abstract}

Joke generation is a difficult task for humans and machines alike. It necessitates context, timing, and wordplay to elicit a laugh. We consider a subclass of 'knock-knock' jokes to simplify the generative and use a variety of straight-forward algorithms to create a small number of jokes with origins from movie scripts and the web.

\section{Introduction}

There have been many successful approaches to joke generation. \footnote{Cai, J., and Ehrhardt, N. (2013). Is This A Joke?. Yoshida K., et al. (2018). Neural Joking Machine: Humorous image captioning. Mihalcea, R. and Strapparava, C. (2006). Learning to Laugh (Automatically): Computational Models for Humor Recognition.} 
These jokes typically follow a certain rigid structure such as call-and-response. But others have trained more intelligent models on large corpuses of data scraped from Reddit or Twitter. These have less associated structure and generally see more mixed results.
We want to consider a less common joke-type in the current literature: the 'knock-knock' joke. Considering this joke type has a couple main advantages.
(i) It follows a rigid structure. To illustrate the format, we annotate the following classic 'knock-knock' joke (not generated).
\begin{center}
\begin{enumerate}
\item[A:] Knock knock.
\item[B:] Who's there?
\item[A:] Cash. [\emph{Token}]
\item[B:] Cash who? [\emph{Token + who $\approx$ search word}]
\item[A:] No thanks, I'll have the peanuts [\emph{Play on search word}]
\end{enumerate}
\end{center}

\noindent (ii) There is a discrete set of 'knock-knock' joke subtypes \footnote{Taylor, J. (2004). Computationally recognizing wordplay in jokes.}.
One of which, as Taylor identifies, is word play on the \emph{token}. We pursue this specific type because it allows us to take a more straightforward, algorithmic approach to generation.
The algorithm looks like:
\begin{center}
    \begin{enumerate}
    \item[(a)] Generate \emph{Token} from dictionary/corpus by identifying \emph{search words} that end in the 'who'/'ew' sound and extracting the root of these words.
    \item[(b)] Use corpus to generate responses based on the \emph{search word}.
    \end{enumerate}
\end{center}


%------------------------------------------------

\section{Methods}

\subsection{Generating Tokens}

We used a corpus of $\sim 300k$ words and found words ending in the following substrings: ["who", "ew", "ewe", "ooo", "ooh", "oo", "hue", "ue", "eau", "eww", "hu"].  We cross-referenced the \emph{tokens} with another English language dictionary. This cross-referencing only generates a list of 9 potential english words. To extend this, while sacrificing a bit of quality, we cross-referenced tokens with a pronunciation dataset based on a CMU pronunciation dictionary with words ending in ["UW0", "UW1", "UW2"]. This outputs a list of about 68 english \emph{search words} but the quality of the tokens took a hit. For example, it generates tokens such as 'sh' corresponding to 'shew'. 

\subsection{Generating Responses}
We decided to generate a response to a joke one of two ways:
\begin{enumerate}[label=(\roman*)]
\item Using a bigram approach learned from a Wikipedia dataset. This meant stringing together similar words to artificially construct a response.
\item Using a 'sentence following' approach from a movie script dataset. This meant finding the occurrence of a word in the movie script data set then returning the next sentence in the dialogue.
This dataset is nice because movie scripts allow for more conversational flow then most other corpuses do. It also includes various colloquialisms making it well-suited for jokes.
\end{enumerate}
%------------------------------------------------

\section{Results}


\begin{center}
\begin{supertabular}{lp{10mm}p{30mm}}
\toprule
token & search word & response\\
\midrule
\multicolumn{3}{r}{\textbf{Bigrams}} \\
\cmidrule(r){1-3}
cash & cashew & cashew apple ii cd of the target for championship at 124. \\
\midrule
\multicolumn{3}{r}{\textbf{Sentence Following (1)}} \\
\cmidrule(r){1-3}
fug & fugue & Fuck that, maybe you're my hallucination. \\
resid & residue & Putty? On both doors? \\
mild & mildew & And is that it? \\
rev & revue & The job's gonna cost you a hundred bucks. \\
\midrule
\multicolumn{3}{r}{\textbf{Sentence Following (2)}} \\
\cmidrule(r){1-3}
wa & wahoo & That's my point! \\
gl & glue & What's it to you? \\
curf & curfew & You think he could still be in town? \\
\bottomrule
\end{supertabular}
\end{center}

%------------------------------------------------

\section{Discussion}

\subsection{Why Human Evaluation}

We decided to do human evaluation because of the nature of the results. It's really hard to evaluate a joke quantitatively using machine-based methods, due to the lack of models about them. This leaves only a few options, among them chiefly is human evaluation. A score generated from collective human reactions to jokes generated may be a good way to evaluate our models.

\subsection{Evaluation Methods}

To test our results, we would have a survey that evaluates for different methods for producing jokes. It would include a few generated by each methods along with a place to score the joke as a whole for its coherence and humor.

\subsection{Evaluation Results}

The survey has not been finished... to be continued.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template


\bibitem[Yoshida, Kota et al, 2018]{Yoshida:2018dg}
Kota Yoshida and Munetaka Minoguchi and Kenichiro Wani and Akio Nakamura and Hirokatsu Kataoka. (2018).
\newblock Neural Joking Machine: Humorous image captioning.
\newblock {\em CoRR}, abs/1805.11850.

\bibitem[Cai, J., and Ehrhardt, N., 2013]{Cai:2013dg}
Cai, J., and Ehrhardt, N. (2013). 
\newblock Is This A Joke?.

\bibitem[Mihalcea, R. and Strapparava, C, 2006]{Mihalcea:2006dg}
Mihalcea, R. and Strapparava, C. (2006), 
\newblock Learning to Laugh (Automatically): Computational Models for Humor Recognition.
\newblock{\em Computational Intelligence}, 22: 126-142. doi:10.1111/j.1467-8640.2006.00278.x

\bibitem[Taylor, 2004]{Taylor:2004dg}
Taylor, J. (2006), 
\newblock Computationally recognizing wordplay in jokes
\newblock{\em Cognitive Science}

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}