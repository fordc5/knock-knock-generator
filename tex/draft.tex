\documentclass[twoside,twocolumn]{article}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} 
\linespread{1.05} 
\usepackage{microtype} 

\usepackage[english]{babel} 

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} 
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} 
\usepackage{booktabs} 

\usepackage{lettrine} 

\usepackage{enumitem} 
\setlist[itemize]{noitemsep} 

\usepackage{abstract} 
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{titlesec} 
\renewcommand\thesection{\Roman{section}} 
\renewcommand\thesubsection{\roman{subsection}} 
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} 
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} 

\usepackage{fancyhdr} 
\pagestyle{fancy} 
\fancyhead{}
\fancyfoot{} 
\fancyhead[C]{Running title $\bullet$ May 2016 $\bullet$ Vol. XXI, No. 1} % Custom header text
\fancyfoot[RO,LE]{\thepage} 

\usepackage{titling} 
\usepackage{hyperref} 

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip}

\pretitle{\begin{center}\Huge\bfseries} 
\posttitle{\end{center}} 
\title{Joke's On You: An Exercise in Joke Generation} 
\author{%
\textsc{Connor Ford and Gabe Magee} \\[1ex] 
\normalsize Pomona College \\ 
\normalsize connor.ford@pomona.edu, gabe.magee@pomona.edu
}
\date{\today}


\begin{document}


\maketitle

\section*{Abstract}

Joke generation is a difficult task for humans and machines alike. 
We consider a subclass of 'knock-knock' jokes to simplify the generative approach.
Using an algorithm as opposed to training and testing more 'intelligently' we are able to create a small number of reasonable jokes with origins from movie scripts.

\section{Introduction}

There have been many successful approaches to joke generation. 
Cai and Ehrhardt \footnote{Cai, J., and Ehrhardt, N. (2013). Is This A Joke?.} tried to distinguish between a non-joke sentence and a joke one using Neural Nets. 
Yoshida et al \footnote{Kota Yoshida and Munetaka Minoguchi and Kenichiro Wani and Akio Nakamura and Hirokatsu Kataoka. (2018). Neural Joking Machine: Humorous image captioning.} took in various image/caption pairs and tried to produce humorous captions given an image. 
Finally, Mihalcea and Strapparva \footnote{Mihalcea, R. and Strapparava, C. (2006). Learning to Laugh (Automatically): Computational Models for Humor Recognition.} tried to apply Linguistic theories about humor to computational generation of one-liner jokes. 
These jokes typically follow a certain structure like call-and-response, or the more vulgar yo-mama. 
Others have trained models on large corpuses of data scraped from reddit or twitter. These have less associated structure and generally see more mixed results.
We wanted to consider a less common joke-type in current literature: the 'knock-knock' joke. This joke type has a couple main advantages.
(i) It follows a rigid structure. To illustrate the format, we annotate the following classic 'knock-knock' joke (not generated).
\begin{center}
\begin{enumerate}
\item[A:] Knock knock.
\item[B:] Who's there?
\item[A:] Cash. [\emph{Token}]
\item[B:] Cash who? [\emph{Token + who = search word}]
\item[A:] No thanks, I'll have the peanuts [\emph{Play on search word}]
\end{enumerate}
\end{center}

\noindent (ii) There is a discrete set of 'knock-knock' joke subtypes \footnote{Taylor, J. (2004). Computationally recognizing wordplay in jokes.}.
One of which, as Taylor identifies, is word play on the \emph{token}. We pursue this specific type because it always us to take a straightfoward, algorithmic approach to generation.
The algorithm looks like:
\begin{center}
    \begin{enumerate}
    \item[(a)] Generate \emph{Token} from dictionary/corpus by identifying \emph{search words} that end in the 'who'/'ew' and extracting the root of these words.
    \item[(b)] Use corpus to generate responses based on \emph{search word}.
    \end{enumerate}
\end{center}


%------------------------------------------------

\section{Methods}

\subsection{Generating Tokens}

Used corpus of $\sim 300k$ words and found words ending in the following characters: ["who", "ew", "ewe", "ooo", "ooh", "oo", "hue", "ue", "eau", "eww", "hu"]
Then we cross-referenced these words with a pronounciation dataset based on a CMU pronounciation dictionary ending in ["UW0", "UW1", "UW2"]. 
This only generated a list of 9 potential english words. To extend this, while sacrificing quality, instead of cross-referencing with the pronounciation library, we sacrificed with another english word dictionary. 
This outputs a list of about 68 english \emph{search words} but the quality of the tokens took a large hit.

\subsection{Generating Responses}

(i) Used bigram approach in Wikipedia dataset.

(ii) Used sentence following approach in movie script dataset.

%------------------------------------------------

\section{Results}


\begin{center}
\begin{tabular}{llr}
\toprule
\multicolumn{2}{c}{Name} \\
\cmidrule(r){1-2}
First name & Last Name & Grade \\
\midrule
John & Doe & $7.5$ \\
Richard & Miles & $2$ \\
\bottomrule
\end{tabular}
\end{center}



\begin{equation}
\label{eq:emc}
e = mc^2
\end{equation}



%------------------------------------------------

\section{Discussion}

\subsection{Subsection One}


\subsection{Subsection Two}



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template


\bibitem[Yoshida, Kota et al, 2018]{Yoshida:2018dg}
Kota Yoshida and Munetaka Minoguchi and Kenichiro Wani and Akio Nakamura and Hirokatsu Kataoka. (2018).
\newblock Neural Joking Machine: Humorous image captioning.
\newblock {\em CoRR}, abs/1805.11850.

\bibitem[Cai, J., and Ehrhardt, N., 2013]{Cai:2013dg}
Cai, J., and Ehrhardt, N. (2013). 
\newblock Is This A Joke?.

\bibitem[Mihalcea, R. and Strapparava, C, 2006]{Mihalcea:2006dg}
Mihalcea, R. and Strapparava, C. (2006), 
\newblock Learning to Laugh (Automatically): Computational Models for Humor Recognition.
\newblock{\em Computational Intelligence}, 22: 126-142. doi:10.1111/j.1467-8640.2006.00278.x

\bibitem[Taylor, 2004]{Taylor:2004dg}
Taylor, J. (2006), 
\newblock Computationally recognizing wordplay in jokes
\newblock{\em Cognitive Science}

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}