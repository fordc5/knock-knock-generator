\documentclass[twoside,twocolumn]{article}

\usepackage[sc]{mathpazo} % Use the Palatino font
\usepackage[T1]{fontenc} 
\linespread{1.05} 
\usepackage{microtype} 
\usepackage{supertabular}
\usepackage[english]{babel} 

\usepackage[hmarginratio=1:1,top=32mm,columnsep=20pt]{geometry} 
\usepackage[hang, small,labelfont=bf,up,textfont=it,up]{caption} 
\usepackage{booktabs} 

\usepackage{lettrine} 

\usepackage{enumitem} 
\setlist[itemize]{noitemsep} 

\usepackage{abstract} 
\renewcommand{\abstractnamefont}{\normalfont\bfseries} 
\renewcommand{\abstracttextfont}{\normalfont\small\itshape}

\usepackage{titlesec} 
\renewcommand\thesection{\Roman{section}} 
\renewcommand\thesubsection{\roman{subsection}} 
\titleformat{\section}[block]{\large\scshape\centering}{\thesection.}{1em}{} 
\titleformat{\subsection}[block]{\large}{\thesubsection.}{1em}{} 

\usepackage{fancyhdr} 
\pagestyle{fancy} 
\fancyhead{}
\fancyfoot{} 
\fancyhead[C]{Joke's On You $\bullet$ May 2019 $\bullet$ Ford, Magee} % Custom header text
\fancyfoot[RO,LE]{\thepage} 

\usepackage{titling} 
\usepackage{hyperref} 

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\setlength{\droptitle}{-4\baselineskip}

\pretitle{\begin{center}\Huge\bfseries} 
\posttitle{\end{center}} 
\title{Joke's On You: An Exercise in Joke Generation} 
\author{%
\textsc{Connor Ford and Gabe Magee} \\[1ex] 
\normalsize Pomona College \\ 
\normalsize connor.ford@pomona.edu, gabe.magee@pomona.edu
}
\date{\today}


\begin{document}


\maketitle

\section*{Abstract}

Joke generation is a difficult task for humans and machines alike. It combines context, timing, wordplay and world knowledge in a way that can get a reaction out of a real human being. Is there a small predefined subset of a joke that can be generated by machines?
We consider a subclass of 'knock-knock' jokes to simplify the generative approach.
Using an algorithm as opposed to training and testing more 'intelligently' we are able to create a small number of reasonable jokes with origins from movie scripts.

\section{Introduction}

There have been many successful approaches to joke generation. 
Cai and Ehrhardt \footnote{Cai, J., and Ehrhardt, N. (2013). Is This A Joke?.} tried to distinguish between a non-joke sentence and a joke one using Neural Nets. 
Yoshida et al \footnote{Kota Yoshida and Munetaka Minoguchi and Kenichiro Wani and Akio Nakamura and Hirokatsu Kataoka. (2018). Neural Joking Machine: Humorous image captioning.} took in various image/caption pairs and tried to produce humorous captions given an image. 
Finally, Mihalcea and Strapparva \footnote{Mihalcea, R. and Strapparava, C. (2006). Learning to Laugh (Automatically): Computational Models for Humor Recognition.} tried to apply Linguistic theories about humor to computational generation of one-liner jokes. 
These jokes typically follow a certain structure like call-and-response, or the more vulgar yo-mama. 
Others have trained models on large corpuses of data scraped from reddit or twitter. These have less associated structure and generally see more mixed results.
We wanted to consider a less common joke-type in current literature: the 'knock-knock' joke. This joke type has a couple main advantages.
(i) It follows a rigid structure. To illustrate the format, we annotate the following classic 'knock-knock' joke (not generated).
\begin{center}
\begin{enumerate}
\item[A:] Knock knock.
\item[B:] Who's there?
\item[A:] Cash. [\emph{Token}]
\item[B:] Cash who? [\emph{Token + who = search word}]
\item[A:] No thanks, I'll have the peanuts [\emph{Play on search word}]
\end{enumerate}
\end{center}

\noindent (ii) There is a discrete set of 'knock-knock' joke subtypes \footnote{Taylor, J. (2004). Computationally recognizing wordplay in jokes.}.
One of which, as Taylor identifies, is word play on the \emph{token}. We pursue this specific type because it always us to take a straightfoward, algorithmic approach to generation.
The algorithm looks like:
\begin{center}
    \begin{enumerate}
    \item[(a)] Generate \emph{Token} from dictionary/corpus by identifying \emph{search words} that end in the 'who'/'ew' and extracting the root of these words.
    \item[(b)] Use corpus to generate responses based on \emph{search word}.
    \end{enumerate}
\end{center}


%------------------------------------------------

\section{Methods}

\subsection{Generating Tokens}

Used corpus of $\sim 300k$ words and found words ending in the following characters: ["who", "ew", "ewe", "ooo", "ooh", "oo", "hue", "ue", "eau", "eww", "hu"]
Then we cross-referenced these words with a pronounciation dataset based on a CMU pronounciation dictionary ending in ["UW0", "UW1", "UW2"]. 
This only generated a list of 9 potential english words. To extend this, while sacrificing quality, instead of cross-referencing with the pronounciation library, we sacrificed with another english word dictionary. 
This outputs a list of about 68 english \emph{search words} but the quality of the tokens took a large hit.

\subsection{Generating Responses}
We decided to generate a response to a joke one of two ways:
\begin{enumerate}[label=(\roman*)]
\item Using a bigram approach learned from a Wikipedia dataset. This meant stringing together similar words to artificially construct a response.
\item Using a 'sentence following' approach from a movie script dataset. This meant finding the occurrence of a word in the movie script data set then returning the next sentence in the dialogue.
This dataset is nice because movie scripts allow for more conversational flow then most other corpuses do. It also includes various colloquialisms making it well-suited for jokes.
\end{enumerate}
%------------------------------------------------

\section{Results}


\begin{center}
\begin{supertabular}{lp{10mm}p{30mm}}
\toprule
token & search word & response\\
\midrule
\multicolumn{3}{r}{\textbf{Bigrams}} \\
\cmidrule(r){1-3}
cash & cashew & cashew apple ii cd of the target for championship at 124. \\
\midrule
\multicolumn{3}{r}{\textbf{Sentence Following (1)}} \\
\cmidrule(r){1-3}
fug & fugue & Fuck that, maybe you're my hallucination. \\
resid & residue & Putty? On both doors? \\
mild & mildew & And is that it? \\
rev & revue & The job's gonna cost you a hundred bucks. \\
\midrule
\multicolumn{3}{r}{\textbf{Sentence Following (2)}} \\
\cmidrule(r){1-3}
wa & wahoo & That's my point! \\
gl & glue & What's it to you? \\
curf & curfew & You think he could still be in town? \\
\bottomrule
\end{supertabular}
\end{center}

%------------------------------------------------

\section{Discussion}

\subsection{Why Human Evaluation}

We decided to do human evaluation because of the nature of the results. It's really hard to evaluate a joke quantitatively using machine-based methods, due to the lack of models about them. This leaves only a few options, among them chiefly is human evaluation. A score generated from collective human reactions to jokes generated may be a good way to evaluate our models.

\subsection{Evaluation Methods}

To test our results, we would have a survey that evaluates for different methods for producing jokes. It would include a few generated by each methods along with a place to score the joke as a whole for its coherence and humor.

\subsection{Evaluation Results}

The survey has not been finished... to be continued.


%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------

\begin{thebibliography}{99} % Bibliography - this is intentionally simple in this template


\bibitem[Yoshida, Kota et al, 2018]{Yoshida:2018dg}
Kota Yoshida and Munetaka Minoguchi and Kenichiro Wani and Akio Nakamura and Hirokatsu Kataoka. (2018).
\newblock Neural Joking Machine: Humorous image captioning.
\newblock {\em CoRR}, abs/1805.11850.

\bibitem[Cai, J., and Ehrhardt, N., 2013]{Cai:2013dg}
Cai, J., and Ehrhardt, N. (2013). 
\newblock Is This A Joke?.

\bibitem[Mihalcea, R. and Strapparava, C, 2006]{Mihalcea:2006dg}
Mihalcea, R. and Strapparava, C. (2006), 
\newblock Learning to Laugh (Automatically): Computational Models for Humor Recognition.
\newblock{\em Computational Intelligence}, 22: 126-142. doi:10.1111/j.1467-8640.2006.00278.x

\bibitem[Taylor, 2004]{Taylor:2004dg}
Taylor, J. (2006), 
\newblock Computationally recognizing wordplay in jokes
\newblock{\em Cognitive Science}

\end{thebibliography}

%----------------------------------------------------------------------------------------

\end{document}